{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBKiDg6kTc6VwLkBajW8qt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NamanGautam0404/NG.pro/blob/main/AI_Role_Assignment_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "L_cXktzhDi93"
      },
      "outputs": [],
      "source": [
        "\n",
        "import gdown\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FaT5UzAWGFOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open video.\")\n",
        "    exit()\n",
        "\n",
        "# Get video properties\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# Define quadrants (adjust coordinates based on your video dimensions)\n",
        "quadrants = {\n",
        "    1: (0, 0, frame_width // 2, frame_height // 2),\n",
        "    2: (frame_width // 2, 0, frame_width, frame_height // 2),\n",
        "    3: (0, frame_height // 2, frame_width // 2, frame_height),\n",
        "    4: (frame_width // 2, frame_height // 2, frame_width, frame_height)\n",
        "}\n",
        "\n",
        "# Initialize video writer for output\n",
        "output_video_path = 'processed_video.avi'\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "output_video = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "# Initialize variables for tracking\n",
        "previous_positions = {color: None for color in colors_to_track}\n",
        "events = []\n",
        "\n",
        "# Process each frame\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert frame to HSV color space for better color detection\n",
        "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Track balls of each color\n",
        "    for color_name, color_value in colors_to_track.items():\n",
        "        lower_bound, upper_bound = color_boundaries[color_name]\n",
        "        lower_bound = np.array(lower_bound, dtype=np.uint8)\n",
        "        upper_bound = np.array(upper_bound, dtype=np.uint8)\n",
        "\n",
        "        # Mask for the color\n",
        "        mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
        "        mask = cv2.erode(mask, None, iterations=2)\n",
        "        mask = cv2.dilate(mask, None, iterations=2)\n",
        "\n",
        "        # Find contours in the mask\n",
        "        contours, _ = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        # Track each contour\n",
        "        for cnt in contours:\n",
        "            area = cv2.contourArea(cnt)\n",
        "            if area > 100:  # Adjust threshold as needed\n",
        "                # Get centroid of the contour\n",
        "                center = centroid(cnt)\n",
        "                if center:\n",
        "                    cx, cy = center\n",
        "                    cv2.circle(frame, (cx, cy), 5, color_value, -1)\n",
        "\n",
        "                    # Determine current quadrant\n",
        "                    current_quadrant = None\n",
        "                    for q_num, q_coords in quadrants.items():\n",
        "                        x1, y1, x2, y2 = q_coords\n",
        "                        if x1 <= cx <= x2 and y1 <= cy <= y2:\n",
        "                            current_quadrant = q_num\n",
        "                            break\n",
        "\n",
        "                    # Check for entry or exit\n",
        "                    if previous_positions[color_name] is None:\n",
        "                        previous_positions[color_name] = current_quadrant\n",
        "\n",
        "                    prev_quadrant = previous_positions[color_name]\n",
        "                    if prev_quadrant != current_quadrant:\n",
        "                        if current_quadrant is not None:\n",
        "                            events.append((cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0, current_quadrant, color_name, 'Entry'))\n",
        "                        if prev_quadrant is not None:\n",
        "                            events.append((cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0, prev_quadrant, color_name, 'Exit'))\n",
        "                        previous_positions[color_name] = current_quadrant\n",
        "\n",
        "                    # Overlay text\n",
        "                    if current_quadrant is not None:\n",
        "                        text = f\"Q{current_quadrant} - {color_name}\"\n",
        "                        cv2.putText(frame, text, (cx - 20, cy - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color_value, 2)\n",
        "\n",
        "    # Write frame to output video\n",
        "    output_video.write(frame)\n",
        "\n",
        "# Release video capture and writer\n",
        "cap.release()\n",
        "output_video.release()\n",
        "\n",
        "# Write events to text file\n",
        "output_txt_path = 'output_events.txt'\n",
        "with open(output_txt_path, 'w') as f:\n",
        "    for event in events:\n",
        "        event_time, quadrant, ball_color, event_type = event\n",
        "        f.write(f\"Time: {event_time:.2f}s, Quadrant: {quadrant}, Ball Colour: {ball_color}, Event Type: {event_type}\\n\")\n",
        "\n",
        "print(\"Processing completed.\")\n",
        "print(f\"Processed Video saved as: {output_video_path}\")\n",
        "print(f\"Event Data saved as: {output_txt_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5pH8q0lF3Kd",
        "outputId": "ce0be81c-664a-42bd-cfb9-ca4203e4dabf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing completed.\n",
            "Processed Video saved as: processed_video.avi\n",
            "Event Data saved as: output_events.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ajh6VlxG3Om"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}